
Welcome to my HomeLab! In this post, I’ll walk you through my journey of setting up a Kubernetes storage solution. I explored two popular options: **local-path-provisioner** and **Ceph Rook**. While both have their strengths, I opted for **Ceph Rook** instead of simple **local-path-provisioner** because of native snapshots and mirroring.

Complete source code for the live cluster is available https://github.com/Rahulsharma0810/HomeLab-Kubernetes

## My Current Setup

Hardware

| SSD | M.2 Predator SSD GM7000 2TB<br>2 X Samsung SSD 860 250GB<br> |
| --- | ------------------------------------------------------------ |

Storage Alignment Across VMs 

| **VM**        | **Disk**                             |
| ------------- | ------------------------------------ |
| Proxmox       | 100 GB                               |
| Control-Plane | 50 GB (For Only K 8 s  Components)   |
| Worker Node   | 200 GB                               |
| Worker Node   | 1600 GB (Raw Disk, Shared From 2 TB) |
| Worker Node   | 2 X Samsung SSD 860 250GB            |
Attaching Disk to Worker Node via promox.
```
ls -n /dev/disk/by-id/
qm set 101 -virtio2 /dev/disk/by-id/wwn-0x5002538e40eb13c9
qm set 101 -virtio3 /dev/disk/by-id/wwn-0x5002538e40eb1435
```

Below is the Talos Linux View of Worker Node.
```
❯ talosctl get disks -n 192.168.0.6
NODE          NAMESPACE   TYPE   ID      VERSION   SIZE     READ ONLY   TRANSPORT   ROTATIONAL   WWID   MODEL           SERIAL
192.168.0.6   runtime     Disk   loop0   1         75 MB    true
192.168.0.6   runtime     Disk   sda     1         215 GB   false       virtio      true                QEMU HARDDISK
192.168.0.6   runtime     Disk   sr0     1         106 MB   false       ata         true                QEMU DVD-ROM
192.168.0.6   runtime     Disk   vda     1         1.6 TB   false       virtio      true
192.168.0.6   runtime     Disk   vdb     1         250 GB   false       virtio      true
192.168.0.6   runtime     Disk   vdc     1         250 GB   false       virtio      true
```
## Main Goal

- VDA - Storing all the PVs in Worker Node's 1.6 TB.
- VDB - Using as Backup Disk For Critical workloads, Like PaperlessNgx, Immich etc.
- VDC - Using as 3 rd point of backups, in case VDA, VDB fails.

Note: Not all K8s PVs will be synced in VDB and VDC, Only critical workloads. Need to find a way to explicitly snapshot some PVs, For example, Immich, Papelessngx etc.

Incase, VDA failed i can reinstall cluster / application and then pull data data from VDB and VDC.


---
## **Ceph Rook: Scalable and Reliable Storage**

Ceph Rook provides enterprise-grade storage with redundancy, scalability, and seamless Kubernetes integration. While it’s a robust solution, setting it up in a single-node cluster presented some unique challenges.

Checkout the live configurations at : https://github.com/Rahulsharma0810/HomeLab-Kubernetes

### Installing Ceph RooK

Follow Flux CD Structure for installing rook and ceph at  https://github.com/Rahulsharma0810/HomeLab-Kubernetes

### Deploying Ceph tools

First get secret

```
❯ k describe secrets rook-ceph-mon
Name:         rook-ceph-mon
Namespace:    rook-ceph
Labels:       <none>
Annotations:  <none>

Type:  kubernetes.io/rook

Data
====
ceph-secret:    40 bytes
ceph-username:  12 bytes
fsid:           36 bytes
mon-secret:     40 bytes

❯ k get secrets rook-ceph-mon -o "jsonpath={.data['ceph-secret']}" | base64 -D
XX+XX==%
❯ k get secrets rook-ceph-mon -o "jsonpath={.data['fsid']}" | base64 -D
3951c4e9-XX-47f4-ac9d-XX%
```

Declare secret in ConfigMap.

```
# HomeLab-Kubernetes/manifests/infrastructure/ceph-cluster/toolbox.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-config
  namespace: rook-ceph
data:
  ceph.conf: |
    [global]
    fsid = 3951c4e9-XX-47f4-ac9d-XX
    mon_host = 192.168.0.6
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  ceph.client.admin.keyring: |
    [client.admin]
        key = XX+XX==
        caps mds = "allow *"
        caps mon = "allow *"
        caps osd = "allow *"
        caps mgr = "allow *"
```

Let's get into the tools pod and verify ceph status.

```
[root@rook-ceph-tools-844b49ddc-58jln /]# ceph status
  cluster:
    id:     3951c4e9-61aa-47f4-ac9d-14a56673a4d5
    health: HEALTH_WARN
            1 MDSs report slow metadata IOs
            Reduced data availability: 4 pgs inactive
            OSD count 0 < osd_pool_default_size 1

  services:
    mon: 1 daemons, quorum a (age 15m)
    mgr: a(active, since 15m)
    mds: 1/1 daemons up, 1 standby
    osd: 0 osds: 0 up, 0 in

  data:
    volumes: 1/1 healthy
    pools:   4 pools, 4 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             4 unknown

[root@rook-ceph-tools-844b49ddc-58jln /]# ceph osd tree
ID  CLASS  WEIGHT  TYPE NAME     STATUS  REWEIGHT  PRI-AFF
-1              0  root default
```

### Ceph Web Dashboard 

I have exposed the url for better observabilty with CF Tunnels, See main repo for tunnel confs.

```
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
```

### Issues & Solutions

#### No OSDs
Ceph Cluster keep giving me Warn, inspite the disks were available to the node.

Right now i am at-least keep trying this 40 th time to fix the error, lol.

Cpeh wants a wiped disk without partitions else it keep showing warning and note disk as OSds.

Below are the different tried i have done during the exercise. 

##### Wipe: Talos Method

```
talosctl reset -n 192.168.0.6 --user-disks-to-wipe /dev/vda,/dev/vdb,/dev/vdc --reboot
```

Node got stopped after this command, had to restart from proxmox again and change the ip, also again applied the node configuration with

```
talosctl apply-config --insecure --nodes 192.168.0.6 --file rendered/worker01.yaml
```

Below doesn't help
```
talosctl disks wipe /dev/vda -n 192.168.0.6
```

##### Wipe: Gparted GUI ISO

booted gparted gui iso to wipe these disk out and then started vm with talos linux iso.

![[attachments/2024-12-17_02-15-41.png]]

##### Wipe: BusyBox pod

Running Command in other pod.
```
  # HomeLab-Kubernetes/manifests/infrastructure/ceph-cluster/ceph-diskwipe.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: disk-wiper
    namespace: rook-ceph
  spec:
    restartPolicy: Never
    containers:
    - name: disk-wiper
      image: busybox
      securityContext:
        privileged: true
        allowPrivilegeEscalation: true
      command:
      - "/bin/sh"
      - "-c"
      - |
        echo "Wiping disks vda, vdb, and vdc..."
        dd if=/dev/zero of=/dev/vda bs=1M count=100 || true
        dd if=/dev/zero of=/dev/vdb bs=1M count=100 || true
        dd if=/dev/zero of=/dev/vdc bs=1M count=100 || true
        echo "Disk wipe completed."
```

Lastly the Issue was with the manifest. (Commented out code was referred via ChatGPT and it is completely wrong, Cluster warn issue solved by removing the commented code.)
```
useAllDevices: true
# nodes:
# - name: I1-1806-Talos-Worker01
#   devices:
#   - name: vda
#   - name: vdb
#   - name: vdc
nodes:
- config:
    osdsPerDevice: "1"
    storeType: bluestore
```

#### HEALTH_WARN: Reduced data availability: 32 pgs inactive, Degraded data redundancy: 32 pgs undersized

As soon as we applied replicatd.Size = 3, Cluster goes to Warn Mode.

File can be read at /manifests/infrastructure/ceph-cluster/replicated-x3-block-store.yaml

```
[root@rook-ceph-tools-0 /]# ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP     META     AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  1.46480   1.00000  1.5 TiB   76 MiB   10 MiB    1 KiB   65 MiB  1.5 TiB  0.00  0.67   92      up
 1    hdd  0.22739   1.00000  233 GiB   46 MiB  1.9 MiB    1 KiB   44 MiB  233 GiB  0.02  2.58   15      up
 2    hdd  0.22739   1.00000  233 GiB   28 MiB  1.8 MiB    1 KiB   26 MiB  233 GiB  0.01  1.58    6      up
                       TOTAL  1.9 TiB  150 MiB   14 MiB  4.7 KiB  135 MiB  1.9 TiB  0.01
MIN/MAX VAR: 0.67/2.58  STDDEV: 0.01
```

```
[root@rook-ceph-tools-0 /]# ceph pg stat
113 pgs: 32 undersized+peered, 81 active+clean; 32 MiB data, 150 MiB used, 1.9 TiB / 1.9 TiB avail; 1.2 KiB/s rd, 2 op/s
```

**Why is This Happening?**

- The replication factor (`size: 3`) requires three copies of each piece of data across different OSDs.
- **Smaller disks** (`vdb` and `vdc`) cannot hold the same amount of data as `vda`, leading to an imbalance
## Final Thoughts

Both **local-path-provisioner** and **Ceph Rook** have their strengths and weaknesses. While local-path-provisioner is lightweight and easy to set up, its lack of built-in redundancy and security concerns made it less suitable for my HomeLab. On the other hand, Ceph Rook provides powerful features like snapshots, scalability, and redundancy, but demands more compute and planning.

In a single-node setup, I intentionally minimised Ceph Rook’s footprint while addressing installation challenges and planning for future risks. It’s not perfect, but it’s a step toward building a resilient and scalable HomeLab.

---

Issue:

```
NAMESPACE   NAME                                                              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
Every 1.0s: kubectl get CephBlockPool,CephFilesystem,cephcluster,storageclasses -A                                                                                                                           MAC-DQ4V1W4NV5: Mon Dec 16 18:12:48 2024

NAMESPACE   NAME                                                   PHASE   TYPE         FAILUREDOMAIN   AGE
rook-ceph   cephblockpool.ceph.rook.io/replicated-x1-block-store   Ready   Replicated   host            80m

NAMESPACE   NAME                                                   ACTIVEMDS   AGE   PHASE
rook-ceph   cephfilesystem.ceph.rook.io/replicated-x1-filesystem   1           75s   Ready

NAMESPACE   NAME                                 DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH        EXTERNAL   FSID
rook-ceph   cephcluster.ceph.rook.io/rook-ceph   /var/lib/rook     1          80m   Ready   Cluster created successfully   HEALTH_WARN              5d23ddef-29fd-451f-b4f8-72c172a6a07d

NAMESPACE   NAME                                                              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
            storageclass.storage.k8s.io/replicated-x1-block-store (default)   rook-ceph.rbd.csi.ceph.com      Retain          Immediate           true                   81m
            storageclass.storage.k8s.io/replicated-x1-cephfs                  rook-ceph.cephfs.csi.ceph.com   Retain          Immediate           true                   75s
```

```
talosctl disks wipe /dev/vdb -n 192.168.0.6
```

X 1 on specific Vda disk (1.5 TB)

```
[root@rook-ceph-tools-844b49ddc-jvc6q /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                        STATUS  REWEIGHT  PRI-AFF
-1         1.91957  root default
-3         1.91957      host i1-1806-talos-worker01
 0    hdd  1.46480          osd.0                        up   1.00000  1.00000
 1    hdd  0.22739          osd.1                        up   1.00000  1.00000
 2    hdd  0.22739          osd.2                        up   1.00000  1.00000
```


Export CrushMap

```
# Export the current CRUSH map
ceph osd getcrushmap -o crushmap.bin

# Decompile the CRUSH map to a text file
crushtool -d crushmap.bin -o crushmap.txt

# Edit the crushmap.txt file
vi crushmap.txt
```

Change block completely.
```
rule replicated-x1-block-store {
    id 1
    type replicated
    step take osd.0
    step emit
}
```

```
# Recompile the CRUSH map
crushtool -c crushmap.txt -o crushmap.bin

# Apply the updated CRUSH map
ceph osd setcrushmap -i crushmap.bin
```

```
ceph osd crush rule dump replicated-x1-block-store
```

Check the values.

---
## **Local Path Provisioner**

The **local-path-provisioner** is a lightweight and simple storage solution for Kubernetes. It’s perfect for single-node clusters where compute and resource efficiency are priorities. But like any solution, it has its trade-offs.

### **Pros**
- **Minimal Components**: Requires very little overhead, ideal for HomeLabs.
- **Fast Performance**: Direct access to the node’s local storage ensures excellent performance.
### **Cons**
1. **No Built-In Backup and Recovery**:
   - You’ll need to configure backups manually or rely on app-native solutions. Luckily, many modern applications support backup and restore. If not, a simple cron job can work wonders.
2. **No Web UI**:
   - Monitoring storage usage requires CLI tools; no visual dashboard is available.
3. **Privileged Mode Concerns**:
   - The provisioner relies on `hostPath` and privileged mode, which can:
     - Increase attack vectors (e.g., privilege escalation).
     - Open the host filesystem to unintended modifications.
     - Bypass Kubernetes security policies, such as `PodSecurityAdmission`.

Given these limitations—and my focus on security—
I decided to take a look to Ceph Rook for better redundancy and management.