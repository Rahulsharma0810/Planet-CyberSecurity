
Welcome to my HomeLab! In this post, I’ll walk you through my journey of setting up a Kubernetes storage solution. I explored two popular options: **local-path-provisioner** and **Ceph Rook**. While both have their strengths, I opted for **Ceph Rook** instead of simple **local-path-provisioner** because of native snapshots and mirroring.

Complete source code for the live cluster is available https://github.com/Rahulsharma0810/HomeLab-Kubernetes

My Current Setup.

| SSD | M.2 Predator SSD GM7000 2TB<br>Samsung SSD 860 250GB<br>Samsung SSD 860 250GB |
| --- | ----------------------------------------------------------------------------- |

I have a 2TB boot disks.
approx. 100GB Goes to Proxmox Installation.
approx. 50GB Goes to Control Plane, where no other workload will be run expect k8s components.

approx 1500GB Goes to Worker Node.

The worker node has 2 X 250GB Samsung Evo Disk for backup up, critical workloads (Primary PV of critical workload will bein 1500GB, because m.2 offer great performance.)

---
## **Ceph Rook: Scalable and Reliable Storage**

Ceph Rook provides enterprise-grade storage with redundancy, scalability, and seamless Kubernetes integration. While it’s a robust solution, setting it up in a single-node cluster presented some unique challenges.

Checkout the live configurations at : https://github.com/Rahulsharma0810/HomeLab-Kubernetes

---
## Main Persistant Disk

Attached Below Disks to worker node 
- 1500 GB (Shared from 2 TB m.2 Disk)
- 2 X 250 GB Samsung Evo (Specially For Backup Critical Loads)

```
❯ talosctl get disks -n 192.168.0.6
NODE          NAMESPACE   TYPE   ID      VERSION   SIZE     READ ONLY   TRANSPORT   ROTATIONAL   WWID   MODEL           SERIAL
192.168.0.6   runtime     Disk   loop0   1         75 MB    true
192.168.0.6   runtime     Disk   sda     1         215 GB   false       virtio      true                QEMU HARDDISK
192.168.0.6   runtime     Disk   sr0     1         106 MB   false       ata         true                QEMU DVD-ROM
192.168.0.6   runtime     Disk   vda     1         1.6 TB   false       virtio      true
192.168.0.6   runtime     Disk   vdb     1         250 GB   false       virtio      true
192.168.0.6   runtime     Disk   vdc     1         250 GB   false       virtio      true
```

Format the disks, before installation of rook-ceph

```
talosctl reset -n 192.168.0.6 --user-disks-to-wipe /dev/vda,/dev/vdb,/dev/vdc --reboot
```

Node got stopped after this command, had to restart from proxmox again and change the ip, also again applied the node configuration with

```
talosctl apply-config --insecure --nodes 192.168.0.6 --file rendered/worker01.yaml
```

Below doesn't help
```
talosctl disks wipe /dev/vda -n 192.168.0.6
```

Lastly i had to boot gparted gui iso to wipe these disk out and then started vm with talos linux iso.

![[attachments/2024-12-17_02-15-41.png]]

#### Deploying Ceph tools

First get secret

```
❯ k describe secrets rook-ceph-mon
Name:         rook-ceph-mon
Namespace:    rook-ceph
Labels:       <none>
Annotations:  <none>

Type:  kubernetes.io/rook

Data
====
ceph-secret:    40 bytes
ceph-username:  12 bytes
fsid:           36 bytes
mon-secret:     40 bytes

❯ k get secrets rook-ceph-mon -o "jsonpath={.data['ceph-secret']}" | base64 -D
XX+XX==%
❯ k get secrets rook-ceph-mon -o "jsonpath={.data['fsid']}" | base64 -D
3951c4e9-XX-47f4-ac9d-XX%

```

Declare secret in configmap.

```
# HomeLab-Kubernetes/manifests/infrastructure/ceph-cluster/toolbox.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-config
  namespace: rook-ceph
data:
  ceph.conf: |
    [global]
    fsid = 3951c4e9-XX-47f4-ac9d-XX
    mon_host = 192.168.0.6
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  ceph.client.admin.keyring: |
    [client.admin]
        key = XX+XX==
        caps mds = "allow *"
        caps mon = "allow *"
        caps osd = "allow *"
        caps mgr = "allow *"
```

Let's get into the tools pod and verify ceph status.

```
[root@rook-ceph-tools-844b49ddc-58jln /]# ceph status
  cluster:
    id:     3951c4e9-61aa-47f4-ac9d-14a56673a4d5
    health: HEALTH_WARN
            1 MDSs report slow metadata IOs
            Reduced data availability: 4 pgs inactive
            OSD count 0 < osd_pool_default_size 1

  services:
    mon: 1 daemons, quorum a (age 15m)
    mgr: a(active, since 15m)
    mds: 1/1 daemons up, 1 standby
    osd: 0 osds: 0 up, 0 in

  data:
    volumes: 1/1 healthy
    pools:   4 pools, 4 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             4 unknown

[root@rook-ceph-tools-844b49ddc-58jln /]# ceph osd tree
ID  CLASS  WEIGHT  TYPE NAME     STATUS  REWEIGHT  PRI-AFF
-1              0  root default
```

Ceph Cluster keep giving me Warn, inspite the disks were available to the node.

Right now i am at-least keep trying this 40th time to fix the error, lol.

All over internet folks keep saying the disks needs to be wiped i did it via below methods.
- GParted Cli (by attaching iso, see above.)
- Running Command in other pod.
  ```
  # HomeLab-Kubernetes/manifests/infrastructure/ceph-cluster/ceph-diskwipe.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: disk-wiper
    namespace: rook-ceph
  spec:
    restartPolicy: Never
    containers:
    - name: disk-wiper
      image: busybox
      securityContext:
        privileged: true
        allowPrivilegeEscalation: true
      command:
      - "/bin/sh"
      - "-c"
      - |
        echo "Wiping disks vda, vdb, and vdc..."
        dd if=/dev/zero of=/dev/vda bs=1M count=100 || true
        dd if=/dev/zero of=/dev/vdb bs=1M count=100 || true
        dd if=/dev/zero of=/dev/vdc bs=1M count=100 || true
        echo "Disk wipe completed."
    ```
- Lastly the Issue was with the manifest. (Commented out code was reffered via chatgpt and it is completely wrong, Cluster warn issue solved by removing the commented code.)
```
useAllDevices: true
# nodes:
# - name: I1-1806-Talos-Worker01
#   devices:
#   - name: vda
#   - name: vdb
#   - name: vdc
nodes:
- config:
    osdsPerDevice: "1"
    storeType: bluestore	
```


---
### **Challenges (What We Fixed During Installation)**

1. **High Default Resource Usage**:
   - By default, Ceph Rook deploys multiple replicas for each component (e.g., monitors, managers, and provisioners), designed for high availability in multi-node clusters. This was overkill for my single-node setup.
   - **Solution**: Adjusted configuration for minimal resource usage:
     - Reduced replication factor to `1`.
     - Set `mon` and `mgr` counts to `1`.
     - Limited CSI provisioner replicas to `1`:
       ```yaml
       csi:
         provisionerReplicas: 1
       ```

2. **Pod Scheduling Conflicts**:
   - Ceph’s anti-affinity rules prevented some pods from being scheduled in a single-node environment.
   - **Solution**: Relaxed anti-affinity rules to allow pods to run on the same node:
     ```yaml
     placement:
       podAntiAffinity:
         preferredDuringSchedulingIgnoredDuringExecution:
         - weight: 100
           podAffinityTerm:
             labelSelector:
               matchExpressions:
               - key: app
                 operator: In
                 values:
                 - rook-ceph-mds
             topologyKey: kubernetes.io/hostname
     ```

3. **Simplifying Resource Requests**:
   - Some Ceph components were consuming more CPU and memory than necessary.
   - **Solution**: Restricted resource limits to match my node’s capacity:
     ```yaml
     resources:
       requests:
         memory: "1024Mi"
         cpu: "500m"
       limits:
         memory: "2048Mi"
         cpu: "1000m"
     ```

---
### **Countermeasures (Future Risks and Solutions)**

1. **Single Point of Failure**:
   - All Ceph services (e.g., MON, MGR, OSD) and workloads run on the same node, meaning a node failure will bring everything down.
   - **Mitigation**:
     - Regularly back up Ceph data to external storage:
       ```bash
       rbd snap create --pool my-pool --image my-volume --snap my-snapshot
       rbd export --pool my-pool my-volume@my-snapshot /backup/location/
       ```
     - Automate exports using tools like `rclone` or custom scripts.

2. **Scaling Limitations**:
   - Expanding to multiple nodes will require reconfiguring the Ceph cluster and increasing the replication factor.
   - **Mitigation**:
     - Plan for future scaling by maintaining modular configuration files.
     - Add additional nodes and update the Ceph configuration:
       ```yaml
       storage:
         useAllNodes: true
         replication:
           size: 2
       ```

3. **Performance Bottlenecks**:
   - Running Ceph and application workloads on the same node could lead to resource contention.
   - **Mitigation**:
     - [ ] Monitor performance using the Ceph dashboard and Prometheus/Grafana.

1. **Manual Backup Management**:
   - While Ceph supports snapshots, external backups still need to be managed manually.
   - **Mitigation**:
     - Automate snapshot management and syncing to an external storage system.


## Issues

The PVC are in pending state when i check logs i saw below.

```
rook-ceph/csi-rbdplugin-provisioner-6767d88746-c6244[csi-provisioner]: I1125 20:16:00.478576 1 event.go:389] "Event occurred" object="cnpg-system/pg-paperless-cluster-1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="failed to provision volume with StorageClass \"replicated-x1-block-store\": error getting secret rook-csi-rbd-provisioner in namespace rook-ceph: secrets \"rook-csi-rbd-provisioner\" not found"
```

While all the Pods were running fine.

```
❯ k get pods
NAME                                            READY   STATUS    RESTARTS   AGE
csi-cephfsplugin-c69cq                          2/2     Running   0          3h44m
csi-cephfsplugin-provisioner-67c8454ddd-g6zp8   5/5     Running   0          3h44m
csi-rbdplugin-d6wr9                             2/2     Running   0          3h44m
csi-rbdplugin-provisioner-6767d88746-c6244      5/5     Running   0          3h44m
rook-ceph-mon-a-5487d449d7-x9xlg                1/1     Running   0          5h9m
rook-ceph-operator-5f4c4bff8d-t8b4b             1/1     Running   0          3h44m
rook-discover-gdqwd                             1/1     Running   0          3h44m
```

It seems because secret are missing the cluster is not behaving as expected.

```
❯ kubectl get secrets -n rook-ceph
NAME                              TYPE                 DATA   AGE
rook-ceph-admin-keyring           kubernetes.io/rook   1      5h11m
rook-ceph-config                  kubernetes.io/rook   2      5h11m
rook-ceph-mon                     kubernetes.io/rook   4      5h11m
rook-ceph-mons-keyring            kubernetes.io/rook   1      5h11m
sh.helm.release.v1.rook-ceph.v1   helm.sh/release.v1   1      3h46m
sh.helm.release.v1.rook-ceph.v2   helm.sh/release.v1   1      137m
```

The secret might not have been created automatically during the Rook-Ceph installation. This can happen if:

1. **The CRDs for Rook-Ceph were not fully initialized.**
2. **The Rook-Ceph cluster is not healthy or fully initialized.**
3. **A misconfiguration in the Rook Helm values or the CephCluster manifest.**

---
## Backups Disk.

I have two 250GB Samsung Evo Disks, that i will be using as backup storage. 

I could Simply mount this in dedicated Node and use them in Minio.

Although a Separate VM shall host VM Minio, but in our case running it on same workload vm makes sense because we only write to Minio on schedule and backup retrieval are weekly operation for integrity test. As well Saving dedicated compute for k8s components on another worker node.

We will be mounting these this in Worker Node.

```
ls -n /dev/disk/by-id/

qm set 101 -virtio2 /dev/disk/by-id/wwn-0x5002538e40eb13c9
qm set 101 -virtio3 /dev/disk/by-id/wwn-0x5002538e40eb1435
```

Results

```
❯ talosctl get disks -n 192.168.0.6
NODE          NAMESPACE   TYPE   ID      VERSION   SIZE     READ ONLY   TRANSPORT   ROTATIONAL   WWID   MODEL           SERIAL
192.168.0.6   runtime     Disk   loop0   1         75 MB    true
192.168.0.6   runtime     Disk   sda     1         215 GB   false       virtio      true                QEMU HARDDISK
192.168.0.6   runtime     Disk   sr0     1         106 MB   false       ata         true                QEMU DVD-ROM
192.168.0.6   runtime     Disk   vda     1         1.6 TB   false       virtio      true
192.168.0.6   runtime     Disk   vdb     1         250 GB   false       virtio      true
192.168.0.6   runtime     Disk   vdc     1         250 GB   false       virtio      true
```

---
## **Final Thoughts**

Both **local-path-provisioner** and **Ceph Rook** have their strengths and weaknesses. While local-path-provisioner is lightweight and easy to set up, its lack of built-in redundancy and security concerns made it less suitable for my HomeLab. On the other hand, Ceph Rook provides powerful features like snapshots, scalability, and redundancy, but demands more compute and planning.

In a single-node setup, I intentionally minimised Ceph Rook’s footprint while addressing installation challenges and planning for future risks. It’s not perfect, but it’s a step toward building a resilient and scalable HomeLab.



---

Issue:

```
NAMESPACE   NAME                                                              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
Every 1.0s: kubectl get CephBlockPool,CephFilesystem,cephcluster,storageclasses -A                                                                                                                           MAC-DQ4V1W4NV5: Mon Dec 16 18:12:48 2024

NAMESPACE   NAME                                                   PHASE   TYPE         FAILUREDOMAIN   AGE
rook-ceph   cephblockpool.ceph.rook.io/replicated-x1-block-store   Ready   Replicated   host            80m

NAMESPACE   NAME                                                   ACTIVEMDS   AGE   PHASE
rook-ceph   cephfilesystem.ceph.rook.io/replicated-x1-filesystem   1           75s   Ready

NAMESPACE   NAME                                 DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH        EXTERNAL   FSID
rook-ceph   cephcluster.ceph.rook.io/rook-ceph   /var/lib/rook     1          80m   Ready   Cluster created successfully   HEALTH_WARN              5d23ddef-29fd-451f-b4f8-72c172a6a07d

NAMESPACE   NAME                                                              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
            storageclass.storage.k8s.io/replicated-x1-block-store (default)   rook-ceph.rbd.csi.ceph.com      Retain          Immediate           true                   81m
            storageclass.storage.k8s.io/replicated-x1-cephfs                  rook-ceph.cephfs.csi.ceph.com   Retain          Immediate           true                   75s
```

```
talosctl disks wipe /dev/vdb -n 192.168.0.6
```


X 1 on specific Vda disk (1.5 TB)

```
[root@rook-ceph-tools-844b49ddc-jvc6q /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                        STATUS  REWEIGHT  PRI-AFF
-1         1.91957  root default
-3         1.91957      host i1-1806-talos-worker01
 0    hdd  1.46480          osd.0                        up   1.00000  1.00000
 1    hdd  0.22739          osd.1                        up   1.00000  1.00000
 2    hdd  0.22739          osd.2                        up   1.00000  1.00000
```


Export CrushMap

```
# Export the current CRUSH map
ceph osd getcrushmap -o crushmap.bin

# Decompile the CRUSH map to a text file
crushtool -d crushmap.bin -o crushmap.txt

# Edit the crushmap.txt file
vi crushmap.txt
```

Change block completely.
```
rule replicated-x1-block-store {
    id 1
    type replicated
    step take osd.0
    step emit
}
```

```
# Recompile the CRUSH map
crushtool -c crushmap.txt -o crushmap.bin

# Apply the updated CRUSH map
ceph osd setcrushmap -i crushmap.bin
```

```
ceph osd crush rule dump replicated-x1-block-store
```

Check the values.



---
## **Local Path Provisioner**

The **local-path-provisioner** is a lightweight and simple storage solution for Kubernetes. It’s perfect for single-node clusters where compute and resource efficiency are priorities. But like any solution, it has its trade-offs.

### **Pros**
- **Minimal Components**: Requires very little overhead, ideal for HomeLabs.
- **Fast Performance**: Direct access to the node’s local storage ensures excellent performance.
### **Cons**
1. **No Built-In Backup and Recovery**:
   - You’ll need to configure backups manually or rely on app-native solutions. Luckily, many modern applications support backup and restore. If not, a simple cron job can work wonders.
2. **No Web UI**:
   - Monitoring storage usage requires CLI tools; no visual dashboard is available.
3. **Privileged Mode Concerns**:
   - The provisioner relies on `hostPath` and privileged mode, which can:
     - Increase attack vectors (e.g., privilege escalation).
     - Open the host filesystem to unintended modifications.
     - Bypass Kubernetes security policies, such as `PodSecurityAdmission`.

Given these limitations—and my focus on security—
I decided to take a look to Ceph Rook for better redundancy and management.