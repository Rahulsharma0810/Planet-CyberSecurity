
Welcome to my HomeLab! In this post, I’ll walk you through my journey of setting up a Kubernetes storage solution. I explored two popular options: **local-path-provisioner** and **Ceph Rook**. While both have their strengths, I opted for Local Path Provisioner instead of Ceph Rook because i encountered mentioned errors in Issues Section.

Complete source code for the live cluster is available [@github/MathiasPius/kronform](https://github.com/MathiasPius/kronform)

---
## **Local Path Provisioner**

The **local-path-provisioner** is a lightweight and simple storage solution for Kubernetes. It’s perfect for single-node clusters where compute and resource efficiency are priorities. But like any solution, it has its trade-offs.

### **Pros**
- **Minimal Components**: Requires very little overhead, ideal for HomeLabs.
- **Fast Performance**: Direct access to the node’s local storage ensures excellent performance.
### **Cons**
1. **No Built-In Backup and Recovery**:
   - You’ll need to configure backups manually or rely on app-native solutions. Luckily, many modern applications support backup and restore. If not, a simple cron job can work wonders.
2. **No Web UI**:
   - Monitoring storage usage requires CLI tools; no visual dashboard is available.
3. **Privileged Mode Concerns**:
   - The provisioner relies on `hostPath` and privileged mode, which can:
     - Increase attack vectors (e.g., privilege escalation).
     - Open the host filesystem to unintended modifications.
     - Bypass Kubernetes security policies, such as `PodSecurityAdmission`.

Given these limitations—and my focus on security—
I decided to take a look to Ceph Rook for better redundancy and management.

---
## **Ceph Rook: Scalable and Reliable Storage**

Ceph Rook provides enterprise-grade storage with redundancy, scalability, and seamless Kubernetes integration. While it’s a robust solution, setting it up in a single-node cluster presented some unique challenges.

Checkout the live configurations at : https://github.com/Rahulsharma0810/HomeLab-Kubernetes

---
### **Challenges (What We Fixed During Installation)**

1. **High Default Resource Usage**:
   - By default, Ceph Rook deploys multiple replicas for each component (e.g., monitors, managers, and provisioners), designed for high availability in multi-node clusters. This was overkill for my single-node setup.
   - **Solution**: Adjusted configuration for minimal resource usage:
     - Reduced replication factor to `1`.
     - Set `mon` and `mgr` counts to `1`.
     - Limited CSI provisioner replicas to `1`:
       ```yaml
       csi:
         provisionerReplicas: 1
       ```

2. **Pod Scheduling Conflicts**:
   - Ceph’s anti-affinity rules prevented some pods from being scheduled in a single-node environment.
   - **Solution**: Relaxed anti-affinity rules to allow pods to run on the same node:
     ```yaml
     placement:
       podAntiAffinity:
         preferredDuringSchedulingIgnoredDuringExecution:
         - weight: 100
           podAffinityTerm:
             labelSelector:
               matchExpressions:
               - key: app
                 operator: In
                 values:
                 - rook-ceph-mds
             topologyKey: kubernetes.io/hostname
     ```

3. **Simplifying Resource Requests**:
   - Some Ceph components were consuming more CPU and memory than necessary.
   - **Solution**: Restricted resource limits to match my node’s capacity:
     ```yaml
     resources:
       requests:
         memory: "1024Mi"
         cpu: "500m"
       limits:
         memory: "2048Mi"
         cpu: "1000m"
     ```

---
### **Countermeasures (Future Risks and Solutions)**

1. **Single Point of Failure**:
   - All Ceph services (e.g., MON, MGR, OSD) and workloads run on the same node, meaning a node failure will bring everything down.
   - **Mitigation**:
     - Regularly back up Ceph data to external storage:
       ```bash
       rbd snap create --pool my-pool --image my-volume --snap my-snapshot
       rbd export --pool my-pool my-volume@my-snapshot /backup/location/
       ```
     - Automate exports using tools like `rclone` or custom scripts.

2. **Scaling Limitations**:
   - Expanding to multiple nodes will require reconfiguring the Ceph cluster and increasing the replication factor.
   - **Mitigation**:
     - Plan for future scaling by maintaining modular configuration files.
     - Add additional nodes and update the Ceph configuration:
       ```yaml
       storage:
         useAllNodes: true
         replication:
           size: 2
       ```

3. **Performance Bottlenecks**:
   - Running Ceph and application workloads on the same node could lead to resource contention.
   - **Mitigation**:
     - [ ] Monitor performance using the Ceph dashboard and Prometheus/Grafana.

1. **Manual Backup Management**:
   - While Ceph supports snapshots, external backups still need to be managed manually.
   - **Mitigation**:
     - Automate snapshot management and syncing to an external storage system.


## Issues

The PVC are in pending state when i check logs i saw below.

```
rook-ceph/csi-rbdplugin-provisioner-6767d88746-c6244[csi-provisioner]: I1125 20:16:00.478576 1 event.go:389] "Event occurred" object="cnpg-system/pg-paperless-cluster-1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="failed to provision volume with StorageClass \"replicated-x1-block-store\": error getting secret rook-csi-rbd-provisioner in namespace rook-ceph: secrets \"rook-csi-rbd-provisioner\" not found"
```

While all the Pods were running fine.

```
❯ k get pods
NAME                                            READY   STATUS    RESTARTS   AGE
csi-cephfsplugin-c69cq                          2/2     Running   0          3h44m
csi-cephfsplugin-provisioner-67c8454ddd-g6zp8   5/5     Running   0          3h44m
csi-rbdplugin-d6wr9                             2/2     Running   0          3h44m
csi-rbdplugin-provisioner-6767d88746-c6244      5/5     Running   0          3h44m
rook-ceph-mon-a-5487d449d7-x9xlg                1/1     Running   0          5h9m
rook-ceph-operator-5f4c4bff8d-t8b4b             1/1     Running   0          3h44m
rook-discover-gdqwd                             1/1     Running   0          3h44m
```

It seems because secret are missing the cluster is not behaving as expected.

```
❯ kubectl get secrets -n rook-ceph
NAME                              TYPE                 DATA   AGE
rook-ceph-admin-keyring           kubernetes.io/rook   1      5h11m
rook-ceph-config                  kubernetes.io/rook   2      5h11m
rook-ceph-mon                     kubernetes.io/rook   4      5h11m
rook-ceph-mons-keyring            kubernetes.io/rook   1      5h11m
sh.helm.release.v1.rook-ceph.v1   helm.sh/release.v1   1      3h46m
sh.helm.release.v1.rook-ceph.v2   helm.sh/release.v1   1      137m
```

The secret might not have been created automatically during the Rook-Ceph installation. This can happen if:

1. **The CRDs for Rook-Ceph were not fully initialized.**
2. **The Rook-Ceph cluster is not healthy or fully initialized.**
3. **A misconfiguration in the Rook Helm values or the CephCluster manifest.**


---
## Backups Disk.

I have two 250GB Samsung Evo Disks, that i will be using as backup storage. 

I could Simply mount this in dedicated Node and use them in Minio.

Although a Separate VM shall host VM Minio, but in our case running it on same workload vm makes sense because we only write to Minio on schedule and backup retrieval are weekly operation for integrity test. As well Saving dedicated compute for k8s components on another worker node.

We will be mounting these this in Worker Node.

```
ls -n /dev/disk/by-id/

qm set 101 -virtio1 /dev/disk/by-id/wwn-0x5002538e40eb13c9
qm set 101 -virtio2 /dev/disk/by-id/wwn-0x5002538e40eb1435
```


---
## **Final Thoughts**

Both **local-path-provisioner** and **Ceph Rook** have their strengths and weaknesses. While local-path-provisioner is lightweight and easy to set up, its lack of built-in redundancy and security concerns made it less suitable for my HomeLab. On the other hand, Ceph Rook provides powerful features like snapshots, scalability, and redundancy, but demands more compute and planning.

In a single-node setup, I intentionally minimised Ceph Rook’s footprint while addressing installation challenges and planning for future risks. It’s not perfect, but it’s a step toward building a resilient and scalable HomeLab.
